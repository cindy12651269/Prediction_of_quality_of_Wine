# -*- coding: utf-8 -*-
"""Keras.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTJaI7DGALy4q7VwSAFMOYLuU4fGQA-D
"""

import tensorflow as tf
from tensorflow.keras import layers, optimizers
from tensorflow.keras.callbacks import EarlyStopping

# Define a custom focal loss function using TensorFlow's backend for efficiency
def focal_loss(alpha=0.25, gamma=2.0):
    def focal_loss_fixed(y_true, y_pred):
        # Clip predictions to prevent log(0) error
        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1.0 - tf.keras.backend.epsilon())
        # Calculate the cross-entropy loss
        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
        # Calculate p_t and the modulating factor (1 - p_t)^gamma
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        modulating_factor = tf.pow((1.0 - p_t), gamma)
        # Final focal loss
        return alpha * modulating_factor * bce
    return focal_loss_fixed

# Define the model with Dropout to prevent overfitting
model = tf.keras.models.Sequential([
    layers.Dense(64, activation="relu", name="hidden1"),
    layers.Dropout(0.3),  # Add dropout for regularization
    layers.Dense(32, activation="relu", name="hidden2"),
    layers.Dense(1, activation="sigmoid", name="output"),
])

# Use Adam optimizer for faster convergence
optimizer = optimizers.Adam(learning_rate=0.001)  # Faster convergence compared to SGD

# Compile the model using the custom focal loss
model.compile(optimizer=optimizer,
              loss=focal_loss(),
              metrics=["accuracy"])

# Define early stopping with lower patience for quicker termination
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Increase batch size for faster training and reduce epochs
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=64, callbacks=[early_stopping])

# Evaluate the model on the test set after training is done
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

y_pred = model.predict(X_test)
y_pred = np.where(y_pred >= 0.5, 1, 0)
print(classification_report(y_test, y_pred, zero_division=1))

y_pred