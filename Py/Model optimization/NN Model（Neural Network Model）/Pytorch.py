# -*- coding: utf-8 -*-
"""PyTorch.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTJaI7DGALy4q7VwSAFMOYLuU4fGQA-D
"""

import torch
import numpy as np
from sklearn.metrics import accuracy_score

# Define the model architecture
class Model(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.hidden1 = torch.nn.Linear(input_size, 64)
        self.hidden2 = torch.nn.Linear(64, 32)
        self.predict = torch.nn.Linear(32, output_size)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = torch.sigmoid(self.predict(x))  # Sigmoid for binary classification
        return x

# Move model and data to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Model(X_train.shape[1], 1).to(device)

# Use Adam optimizer for faster convergence
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_func = torch.nn.BCELoss()

# Convert data to tensors and move to device (GPU/CPU)
x_data = torch.tensor(X_train.values, dtype=torch.float32).to(device)
y_data = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)  # Ensure .values is used

x_val_data = torch.tensor(X_val.values, dtype=torch.float32).to(device)
y_val_data = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)  # Ensure .values is used

x_test_data = torch.tensor(X_test.values, dtype=torch.float32).to(device)
y_test_data = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)  # Ensure .values is used

# Convert target variables to 0 and 1
y_data = (y_data >= 5).float() # Assuming values >= 5 are considered class 1
y_val_data = (y_val_data >= 5).float()
y_test_data = (y_test_data >= 5).float()

# Set batch size and number of epochs
batch_size = 64  # Increased batch size for faster training
num_epochs = 100  # Reduced epochs since Adam converges faster
patience = 10

min_val_loss = np.inf
epochs_no_improve = 0

training_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training loop
    model.train()
    for i in range(0, len(x_data), batch_size):
        batch_x = x_data[i:i+batch_size]
        batch_y = y_data[i:i+batch_size]

        pred = model(batch_x)
        loss = loss_func(pred, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Record training loss
    model.eval()
    with torch.no_grad():
        training_loss = loss_func(model(x_data), y_data)
        training_losses.append(float(training_loss))

        val_pred = model(x_val_data)
        val_loss = loss_func(val_pred, y_val_data)
        val_losses.append(float(val_loss))

        val_acc = accuracy_score(y_val_data.cpu(), (val_pred.cpu() >= 0.5).int())
        print(f"Epoch {epoch+1}, Training Loss: {training_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}")

    # Early stopping
    if val_loss < min_val_loss:
        min_val_loss = val_loss
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1

    if epochs_no_improve >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

y_pred = model(x_test_data)
y_pred = np.where(y_pred >= 0.5, 1, 0)
print(classification_report(y_test, y_pred, zero_division=1))

plt.plot(training_losses)
plt.plot(val_losses)
plt.legend(("Training loss", "val loss"))
plt.xlabel("epoch")
plt.ylabel("loss")